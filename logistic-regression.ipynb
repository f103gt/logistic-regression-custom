{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2c1f771",
   "metadata": {},
   "source": [
    "**Logistic Regression**\n",
    "- Used for supervised learning (When using labeled data - you are performing supervised learning, in all other cases - unsupervised one (clustering))\n",
    "\n",
    "- Classification model (diabetes, spam emails, user is fake or not)\n",
    "\n",
    "- Best for binary classification (you can also do class classification problems, but you can would need to add some more features to it)\n",
    "\n",
    "- Uses sigmoid function\n",
    "\n",
    "\n",
    "Sigmoid curve equation\n",
    "\n",
    "The logistic regression model predicts the probability that a given input $\\mathbf{x}$ belongs to a particular class (e.g., class 1). The formula for the logistic regression model is:\n",
    "\n",
    "$$\n",
    "P(y=1|\\mathbf{x}) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_n x_n)}}\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $P(y=1|\\mathbf{x})$ is the probability that the dependent variable $y$ equals 1 given the input $\\mathbf{x}$.  \n",
    "- $\\beta_0$ is the intercept.  \n",
    "- $\\beta_1, \\beta_2, \\ldots, \\beta_n$ are the coefficients for the independent variables $x_1, x_2, \\ldots, x_n$.  \n",
    "- $e$ is the base of the natural logarithm.\n",
    "\n",
    "\n",
    "In other words the formula for can be represented as following:\n",
    "$$\n",
    "P(y=1|\\mathbf{x}) = \\frac{1}{1 + e^{-(z)}}\n",
    "$$\n",
    "\n",
    "Where $z$\n",
    "\n",
    "$$\n",
    "z = wX + b\n",
    "$$\n",
    "\n",
    "Derivatives:\n",
    "First derivative\n",
    "$$\n",
    "dw = \\frac{1}{m} \\sum_{i=1}^{m} ( \\hat{Y}i - Y_i ) X_i\n",
    "$$\n",
    "\n",
    "Second derivative\n",
    "$$\n",
    "dw = \\frac{1}{m} \\sum_{i=1}^{m} ( \\hat{Y}i - Y_i )\n",
    "$$\n",
    "\n",
    "Gradient Descent:\n",
    "$$\n",
    "w = w - ɑ*dw\n",
    "$$\n",
    "\n",
    "$$\n",
    "b = b - ɑ*db\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65845102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51aea9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir = \"./cell_images\"\n",
    "parasitized_images = \"./cell_images/Parasitized\"\n",
    "uninfected_data = \"./cell_images/Uninfected\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28e59cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_files_in_directories(root_dir):\n",
    "  for dirpath, dirnames, filesnames in os.walk(root_dir):\n",
    "    print(f\"{dirpath}: {len(filesnames)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a7611d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./cell_images/Parasitized: 13780 files\n"
     ]
    }
   ],
   "source": [
    "count_files_in_directories(parasitized_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4e6acfcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./cell_images/Uninfected: 13780 files\n"
     ]
    }
   ],
   "source": [
    "count_files_in_directories(uninfected_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7659f05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIGURATION = {\n",
    "    \"BATCH_SIZE\": 32,\n",
    "    \"IM_SIZE\": 64,\n",
    "    \"IM_DIVISION\": 255.0,\n",
    "    \"N_EPOCHS\": 20,\n",
    "    \"LEARNING_RATE\": 1e-4,\n",
    "    \"N_DENSE_1\": 1024,\n",
    "    \"N_DENSE_2\": 128,\n",
    "    \"CLASS_NAMES\": [\"parasitized\",\"uninfected\"],\n",
    "    \"SPLIT\": 0.2,\n",
    "    \"CLASS_NUM\": 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75025b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27558 files belonging to 2 classes.\n",
      "Using 22047 files for training.\n",
      "Found 27558 files belonging to 2 classes.\n",
      "Using 5511 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    images_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    image_size=(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"]),\n",
    "    batch_size=CONFIGURATION[\"BATCH_SIZE\"],\n",
    "    validation_split=CONFIGURATION[\"SPLIT\"],\n",
    "    subset='training',\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "val_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    images_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    image_size=(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"]),\n",
    "    batch_size=CONFIGURATION[\"BATCH_SIZE\"],\n",
    "    validation_split=CONFIGURATION[\"SPLIT\"],\n",
    "    subset='validation',\n",
    "    seed=123\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "92e0ad81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the images\n",
    "def normalize(image, label):\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image, label\n",
    "\n",
    "train_dataset = train_dataset.map(normalize)\n",
    "val_dataset = val_dataset.map(normalize)\n",
    "\n",
    "# Prefetch the datasets for better performance\n",
    "train_dataset = train_dataset.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8ea01dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the TensorFlow dataset to NumPy arrays\n",
    "def dataset_to_numpy(dataset):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for image, label in dataset:\n",
    "        images.append(image.numpy())\n",
    "        labels.append(label.numpy())\n",
    "      # Concatenate all batches into single NumPy arrays\n",
    "    images = np.concatenate(images, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "X_train, Y_train = dataset_to_numpy(train_dataset)\n",
    "X_val, Y_val = dataset_to_numpy(val_dataset)\n",
    "\n",
    "# Flatten the images\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_val = X_val.reshape(X_val.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "16b3064d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    # declaring learning rate & number of iterations (Hyper parameters)\n",
    "    def __init__(self,learning_rate = 0.01, n_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "\n",
    "        \n",
    "    # fit function to train the model with dataset \n",
    "    # X and Y are matrixes\n",
    "    def fit(self,X,Y):\n",
    "\n",
    "        # number of data points in the dataset (number of rows) --> m\n",
    "        # number of input features in the dataset (number of columns) --> n\n",
    "        self.m, self.n = X.shape \n",
    "\n",
    "        # initiating weight and bias values\n",
    "        self.w = np.zeros(self.n)\n",
    "        self.b = 0\n",
    "\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "        # implement Gradient Descent for Optimization\n",
    "        for i in range(self.n_iterations):\n",
    "            self.__update_weights()\n",
    "\n",
    "    def __update_weights(self):\n",
    "        # Y_hat formula (sigmoid function)\n",
    "        # np.dot - matrix multiplication\n",
    "        # z = xw + b\n",
    "        z = self.X.dot(self.w) + self.b\n",
    "        Y_hat = 1 / (1 + np.exp(-z))\n",
    "\n",
    "        # derivatives (gradients)\n",
    "        # m - number of samples (rows)\n",
    "        # n - number of features (columns) \n",
    "        # use X transpose because\n",
    "        # X dimensions m x n\n",
    "        # w dimensions n x 1\n",
    "        # Y dimensions m x 1\n",
    "        # Y_hat dimensions m x 1\n",
    "        dw = (1/self.m) * np.dot(self.X.T, (Y_hat - self.Y) )\n",
    "        # np.sum - matrix summation\n",
    "        db = (1/self.m) * np.sum(Y_hat - self.Y)\n",
    "\n",
    "        # Gradient Descent \n",
    "        # updating wights and bias using Gradient Descent equation\n",
    "        # w = w - a*dw\n",
    "        # b = b - a*db\n",
    "        self.w = self.w - self.learning_rate * dw\n",
    "        self.b = self.b - self.learning_rate * db\n",
    "\n",
    "\n",
    "    # Sigmoid Equation & Decision Boundary\n",
    "    def predict(self, X):\n",
    "        # Sigmoid Equation\n",
    "        z = X.dot(self.w) + self.b\n",
    "        Y_pred = 1 / (1 + np.exp(-z))\n",
    "        Y_pred = np.where( Y_pred > 0.5, 1, 0)\n",
    "        return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ca2cf2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "00cc9eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8dad1a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO how can i even use my custom LogisticRegression on malaria dataset\n",
    "model.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "61548206",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "330f9bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy_loss(Y_val,Y_pred, epsilon=1e-15):\n",
    "\n",
    "    Y_pred = np.clip(Y_pred, epsilon, 1 - epsilon)\n",
    "    Y_val = np.clip(Y_pred, epsilon, 1 - epsilon)\n",
    "\n",
    "    loss = - np.mean(Y_val * np.log(Y_pred) + (1 - Y_val) * np.log(1 - Y_pred))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b662c43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = binary_cross_entropy_loss(Y_val=Y_val, Y_pred=Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "67ec016b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5535700627826245e-14\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_image_classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
